{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human']\n",
    "    }\n",
    "    def __init__(self):\n",
    "        self.T_min = 20\n",
    "        self.T_max = 80\n",
    "        self._max_episode_steps = 60 + 1\n",
    "        self.T = self.T_max - self.T_min\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1000,\n",
    "            shape = (1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=100,\n",
    "            shape = (1,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.reset()\n",
    "     \n",
    "    def u(self,c):\n",
    "        gamma = 2\n",
    "        return (np.float_power(c, 1-gamma) - 1)/(1 - gamma)\n",
    "    \n",
    "    def step(self, action):\n",
    "        c = action\n",
    "        r = 0.02\n",
    "        done = bool(\n",
    "            self.age == self.T_max\n",
    "        )\n",
    "        self.age += 1\n",
    "        reward = 0\n",
    "        if done:\n",
    "            reward = 2*self.u(c)\n",
    "        else:\n",
    "            reward = self.u(c)\n",
    "        if action > self.state or action <= 0:\n",
    "            reward = -np.inf\n",
    "            \n",
    "        self.state = (self.state - c)*(1+r)\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.age = self.T_min\n",
    "        self.state = np.random.randint(60,80)\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self):\n",
    "        plt.plot(self.age, self.state, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward 0.33333333333333326\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVD0lEQVR4nO3dfaxkZX3A8e8PkOqKKeyy0hXcXazEl5iCekuxvtQKtkiIoDGtZrVbi65GjGhr61tSX1pSbWzx5Q/NVdFtcqUqarGGtBJq1DSW5i6iLFIFlV3BBS4gEF2juPvrH+dcmJ09d3funHk558z3k9zMzDNnZp6TvfO7v/09z3meyEwkSd1yxLQ7IEkaPYO7JHWQwV2SOsjgLkkdZHCXpA46atodADj++ONz8+bN0+6GJLXKjh077srM9VXPNSK4b968mcXFxWl3Q5JaJSJ2rfScZRlJ6iCDuyR1kMFdkjrI4C5JHWRwl6QOMrhL0jQsLMDmzXDEEcXtwsJI374RUyElaaYsLMC2bbB3b/F4167iMcCWLSP5CDN3SZq0d7zjocC+bO/eon1EDO6SNGm7d6+ufQgGd0matI0bV9c+BIO7JE3axRfDmjUHtq1ZU7SPiMFdksatf2YMwPw8bNoEEcXt/PzIBlPB2TKSNF4rzYyZn4dbbhnbx5q5S9I4TWBmTBWDuySN0wRmxlQxuEvSOE1gZkwVg7skjVL/4Ok554x9ZkwVg7skjcry4OmuXZBZ3G7fDlu3jnVmTBVny0jSqKw0eHrllWOdGVPFzF2SRmVKg6dVDhvcI+LSiLgzInb2tK2NiKsi4qby9riy/bkRcV9EXFf+/O04Oy9JjTKlwdMqg2TunwLO7mt7K3B1Zp4CXF0+XvaNzDyt/HnPaLopSS0wgWUFBnXY4J6ZXwfu6Ws+D9he3t8OnD/abklSC0xhWYFBDTugekJm7inv3w6c0PPcMyLi28BPgDdn5g1VbxAR24BtABun8F8WSaplSssKDKr2gGpmJpDlw2uBTZl5KvBh4N8O8br5zJzLzLn169fX7YYkTdaUlhUY1LDB/Y6I2ABQ3t4JkJn3Z+bPyvtXAg+LiONH0lNJapIGzYypMmxw/xKwtby/FbgCICJ+KyKivH96+f531+2kJDVOg2bGVBlkKuRlwDeBJ0TErRFxAfBe4PkRcRNwVvkY4CXAzrLm/iHgpWXZRpLaq3/gdGGhUTNjqkQTYu/c3FwuLi5OuxuSdLD+gVMogvj8fHH/He8oSjEbNxaBfYIzYyJiR2bOVT5ncJekQ9i8uZgJ02/TpqnPijlUcHf5AUk6lIYPnK7E4C5Jh9LwgdOVGNwlqVdD1mOvy+AuScsatB57Xa7nLknLGrQee11m7pK0rKWDp1UM7pK0rKWDp1UM7pJmV0cGT6sY3CXNpg4NnlZxQFXSbOrQ4GkVM3dJs6lDg6dVDO6SZlOHBk+rGNwlzYYOD55WMbhL6r6OD55WcUBVUvd1fPC0ipm7pO7r+OBpFYO7pO7r+OBpFYO7pO6ZscHTKgZ3Sd0yg4OnVRxQldQtMzh4WsXMXVK3zODgaZXDBveIuDQi7oyInT1tayPiqoi4qbw9rmyPiPhQRNwcEd+JiKeNs/OSdJAZHDytMkjm/ing7L62twJXZ+YpwNXlY4AXAKeUP9uAj4ymm5K0AgdPKx02uGfm14F7+prPA7aX97cD5/e0/0sW/gc4NiI2jKivknQgB09XNOyA6gmZuae8fztwQnn/RODHPcfdWrbtoU9EbKPI7tk4Y/9dkjQiDp6uqPaAamYmkEO8bj4z5zJzbv369XW7IWkWOXi6omGD+x3L5Zby9s6y/TbgsT3HnVS2SVJ9/fX1tWurj7MaMHRw/xKwtby/Fbiip/3PylkzZwD39ZRvJGl4VfX1+++Ho48+8LgZHDytMshUyMuAbwJPiIhbI+IC4L3A8yPiJuCs8jHAlcAPgZuBjwGvG0uvJc2eqvr6Aw/Aox4184OnVQ47oJqZL1vhqTMrjk3gwrqdkqSDrFRHv+ceuOuuyfalBbxCVVI7eHHSqhjcJTWTFyfVYnCX1DxenFSbq0JKah4vTqrNzF1S83hxUm0Gd0nT58VJI2dZRtJ0LdfXl8swu3bBwx5WXJz0q189dJyDp6ti5i5purw4aSzM3CVNlxcnjYWZu6Tp8uKksTC4S5osL06aCIO7pMnx4qSJseYuaXK8OGlizNwlTY4XJ02MwV3S+Hhx0tRYlpE0Hl6cNFVm7pLGw4uTpsrMXdJ4eHHSVJm5SxoN6+uNYuYuqT7r641j5i6pPuvrjVMrc4+Ii4BXAwF8LDM/EBHvKtuWysPenplX1uqlpGazvt44Q2fuEfEUiiB+OnAqcG5EPL58+pLMPK38MbBLXWN9vfHqZO5PAq7JzL0AEfE14MUj6ZWk5rK+3gp1au47gWdHxLqIWAOcAzy2fO71EfGdiLg0Io6renFEbIuIxYhYXFpaqjpEUhNZX2+FyMzhXxxxAfA64OfADcAvgX8A7gIS+DtgQ2b+xaHeZ25uLhcXF4fuh6QJOuKIYkXHfhGwf//k+zPDImJHZs5VPVdrtkxmfiIzn56ZzwF+Cnw/M+/IzH2ZuR/4GEVNXlJXuLlGK9QK7hHx6PJ2I0W9/dMRsaHnkBdRlG8ktZWba7RS3YuYPh8R64AHgAsz896I+HBEnEZRlrkFeE3Nz5A0LVWDp8uba1x5ZTEFcuPGIrBbX2+UWsE9M59d0faKOu8pqUHcXKO1vEJVUqG//LKw4OYaLebaMpKqyy/bthUXJ91998HHO3jaeGbuklYuv4CDpy1lcJd06LVh5ue9OKmFDO7SLFrN2jBbthSDp/v3F7cG9law5i7NGteGmQlm7tKscW2YmWDmLs0a116fCWbuUte59vpMMnOXusz6+swyc5e6zPr6zDJzl7rM+vrMMnOXusT6ukpm7lJXWF9XDzN3qSusr6uHmbvUFdbX1cPMXWor6+s6BDN3qY2sr+swzNylNrK+rsMwc5faYGGhCOjLG1Lv2lV9nPV1lQzuUtNVlWAiIPPgY62vq2RZRmq6qhJMZhHge1lfV49awT0iLoqInRFxQ0S8sWxbGxFXRcRN5e1xI+mpNKtWmuKYaX1dKxo6uEfEU4BXA6cDpwLnRsTjgbcCV2fmKcDV5WNJgxp0iuOmTW5/pxXVydyfBFyTmXsz89fA14AXA+cB28tjtgPn1+qhNEuW6+u7dhWZ+a5dcP/9xRTHXpZgdBh1gvtO4NkRsS4i1gDnAI8FTsjMPeUxtwMnVL04IrZFxGJELC4tLdXohtQhTnHUiAw9WyYzb4yI9wFfAX4OXAfs6zsmI6JiSB8ycx6YB5ibm6s8Ruo8pzhqTGoNqGbmJzLz6Zn5HOCnwPeBOyJiA0B5e2f9bkodVFWC6Z8Bs8wpjlqlurNlHl3ebqSot38a+BKwtTxkK3BFnc+QOsspjhqjuvPcPx8R3wX+HbgwM+8F3gs8PyJuAs4qH0vq5xRHjVGtK1Qz89kVbXcDZ9Z5X6mT+uvra9fC3XcffNzyFEepBpcfkCbBVRw1YS4/IE2CUxw1YWbu0jg4xVFTZnCXRs1VHNUAlmWkUXOKoxrA4C7V1b/Q10olGKc4aoIsy0h1rKYE4xRHTZCZu1SHJRg1lMFdWg1LMGoJyzLSoCzBqEXM3KVBWYJRixjcpUG50JdaxOAurcS9TNVi1tylKi70pZYzc5equNCXWs7MXQIX+lLnGNwlF/pSB1mWkZziqA4yuGu29M+AWVhwiqM6ybKMZkdV+WXbNvcyVSeZuWt2VJVflh+vWXNguyUYtZzBXd016CJf99xTlFwswahDapVlIuJNwKuABK4HXgl8FPgD4L7ysD/PzOvqfI60aqudAbNli8FcnTJ0cI+IE4E3AE/OzF9ExGeBl5ZP/3VmXj6KDkpDOdQMmN4Ab/lFHVW3LHMU8IiIOApYA/ykfpekIbjOunSAoYN7Zt4GvB/YDewB7svMr5RPXxwR34mISyLiN6peHxHbImIxIhaXlpaG7Yb0UAlm164ieC+XYKq4yJdmxNDBPSKOA84DTgYeAzwyIl4OvA14IvC7wFrgLVWvz8z5zJzLzLn169cP2w3Nov4s/aKLvAhJ6lOnLHMW8KPMXMrMB4AvAL+fmXuy8Evgk8Dpo+ioBFRn6VVz1MESjGZandkyu4EzImIN8AvgTGAxIjZk5p6ICOB8YGf9bkqlqoHSlXgRkmZYnZr7NcDlwLUU0yCPAOaBhYi4vmw7Hvj7EfRTs2rQgdJ+lmA042rNc8/MdwLv7Gt+Xp33lB60mrnq69bBMcc8tGTvxRdbgtFMc20ZNddq5qp/8IMGc6mHyw+oOZyrLo2MmbuaYTUlGAdKpcMyc1czuGGGNFIGd02HJRhprCzLaPIswUhjZ+au8XO5AGniDO4aL5cLkKbCsozGy+UCpKkwc9douVyA1AgGd43OatZVX7fOEow0RpZlNLyFhaLssryey89+5nIBUkOYuWs4DpRKjWbmruE4UCo1mpm7BuNAqdQqBncdngOlUutYltHBHCiVWs/MXQdyoFTqBDN3HciBUqkTzNxnnQOlUicZ3GeZA6VSZ1mWmSUOlEozo1bmHhFviogbImJnRFwWEQ+PiJMj4pqIuDkiPhMRR4+qs6rBgVJppgwd3CPiROANwFxmPgU4Engp8D7gksx8PPBT4IJRdFSr0F9HX87YVztQun9/cWtgl1qnbs39KOAREXEUsAbYAzwPuLx8fjtwfs3P0GpUZejLjwfhQKnUCUMH98y8DXg/sJsiqN8H7ADuzcxfl4fdCpxY9fqI2BYRixGxuLS0NGw3NMgWdnv3wpFHVr/egVKpk4YeUI2I44DzgJOBe4HPAWcP+vrMnAfmAebm5ip2RtZhVW00vZJ9+4qsvDfwO1AqdVadssxZwI8ycykzHwC+ADwTOLYs0wCcBNxWs49ayWrr6PPzZunSjKgT3HcDZ0TEmogI4Ezgu8BXgZeUx2wFrqjXRT2o7gVHW7Y4UCrNiDo192soBk6vBa4v32seeAvwlxFxM7AO+MQI+ikvOJK0CrUuYsrMdwLv7Gv+IXB6nfcVXnAkqRaXH2giLziSVJPLDzTBIFn6SlyZUVIFg/u0rWY6Yz8vOJK0AssykzbIRUcrcaBU0oDM3CepbpbuQKmkAZm5j5NZuqQpMXMfF7N0SVNk5j4qZumSGsTMfRTM0iU1jJn7MMzSJTWcmftqmaVLagEz98MxS5fUQmbuh2KWLqmlzNx7maVL6ggz92Vm6ZI6ZHYzd7N0SR02G5l7/5K655wD27ebpUvqrO4H96pyy0c/euDuRYeybh0cc8xDfxiW9yKVpAbrVlmmv9SynLFXbU83iOUs3U2lJbVMdzL3qgy99/EgzNIldUR7g/sgW9Pt3QtHHgn79h38ejeWltRhQ5dlIuIJEXFdz8/9EfHGiHhXRNzW037OKDsMrG4D6X37isDda80aeO1rnfEiqbOGztwz83vAaQARcSRwG/BF4JXAJZn5/lF0sFJVHX0lmzYV5ZXeLN9yi6SOG1VZ5kzgB5m5KyJG9JaHsHv3YMctbyC9ZYvBXNJMGdVsmZcCl/U8fn1EfCciLo2I40b0GQ/ZuLG63YuLJAkYQXCPiKOBFwKfK5s+Avw2RclmD/BPK7xuW0QsRsTi0tLS6j704our6+hOW5QkYDSZ+wuAazPzDoDMvCMz92XmfuBjwOlVL8rM+cycy8y59evXr+4Tt2wpsnKzdEmqNIqa+8voKclExIbM3FM+fBGwcwSfcTDr6JK0olrBPSIeCTwfeE1P8z9GxGlAArf0PSdJmoBawT0zfw6s62t7Ra0eSZJq69baMpIkwOAuSZ1kcJekDoocdPnbcXYiYgkYZMeM44G7xtydSfFcmqkr59KV8wDP5VA2ZWblXPJGBPdBRcRiZs5Nux+j4Lk0U1fOpSvnAZ7LsCzLSFIHGdwlqYPaFtznp92BEfJcmqkr59KV8wDPZSitqrlLkgbTtsxdkjQAg7skdVBjg3tEPDYivhoR342IGyLiorJ9bURcFRE3lbej3wxkxCLi4RHxvxHx7fJc3l22nxwR10TEzRHxmXJt/MaLiCMj4lsR8eXycVvP45aIuL7c63exbGvd7xdARBwbEZdHxP9FxI0R8Yy2ncsh9mVu1Xksi4g3ld/3nRFxWRkHJvZdaWxwB34N/FVmPhk4A7gwIp4MvBW4OjNPAa4uHzfdL4HnZeapFJuYnB0RZwDvo9hv9vHAT4ELptfFVbkIuLHncVvPA+APM/O0nrnHbfz9Avgg8B+Z+UTgVIp/n1adS2Z+r/y3OA14OrCXYl/mVp0HQEScCLwBmMvMpwBHUuxYN7nvSma24ge4gmJ54e8BG8q2DcD3pt23VZ7HGuBa4PcorlQ7qmx/BvCf0+7fAP0/ieIL9jzgy0C08TzKvt4CHN/X1rrfL+A3gR9RTpBo87n09P2PgP9u63kAJwI/BtZSrL77ZeCPJ/ldaXLm/qCI2Aw8FbgGOCEf2gzkduCEafVrNcpSxnXAncBVwA+AezPz1+Uht1L8QjTdB4C/AfaXj9fRzvOAYs+Br0TEjojYVra18ffrZGAJ+GRZLvt4uddCG89lWe++zK07j8y8DXg/sJtiu9H7gB1M8LvS+OAeEccAnwfemJn39z6XxZ+/VszlzGLrwdMoMt/TgSdOt0erFxHnAndm5o5p92VEnpWZT6PYKvLCiHhO75Mt+v06Cnga8JHMfCrwc/pKFy06l6p9mR/UlvMoxwXOo/jD+xjgkcDZk+xDo4N7RDyMIrAvZOYXyuY7ImJD+fwGiky4NTLzXuCrFP8lOzYiljdMOQm4bVr9GtAzgRdGxC3Av1KUZj5I+84DeDC7IjPvpKjtnk47f79uBW7NzGvKx5dTBPs2ngv07ctMO8/jLOBHmbmUmQ8AX6D4/kzsu9LY4B4RAXwCuDEz/7nnqS8BW8v7Wylq8Y0WEesj4tjy/iMoxg5upAjyLykPa/y5ZObbMvOkzNxM8d/m/8rMLbTsPKDYIjIiHrV8n6LGu5MW/n5l5u3AjyPiCWXTmcB3aeG5lA7Yl5l2nsdu4IyIWFPGsuV/k4l9Vxp7hWpEPAv4BnA9D9V3305Rd/8ssJFimeA/ycx7ptLJAUXE7wDbKUbMjwA+m5nviYjHUWTAa4FvAS/PzF9Or6eDi4jnAm/OzHPbeB5ln79YPjwK+HRmXhwR62jZ7xdAFPsWfxw4Gvgh8ErK3zVadC7lH9rdwOMy876yra3/Ju8G/pRi5t+3gFdR1Ngn8l1pbHCXJA2vsWUZSdLwDO6S1EEGd0nqIIO7JHWQwV2SOsjgLkkdZHCXpA76fxCV/xqTvkyAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = AgentEnv()\n",
    "env.reset()\n",
    "for i in range(100):\n",
    "    state, reward, done, _ = env.step(1.2)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Reward\", reward)\n",
    "        print(\"Done!\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 16        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/100, score: 61, e: 1.0\n",
      "episode: 1/100, score: 61, e: 1.0\n",
      "episode: 2/100, score: 61, e: 1.0\n",
      "episode: 3/100, score: 61, e: 1.0\n",
      "episode: 4/100, score: 61, e: 1.0\n",
      "episode: 5/100, score: 61, e: 1.0\n",
      "episode: 6/100, score: 61, e: 1.0\n",
      "episode: 7/100, score: 61, e: 1.0\n",
      "episode: 8/100, score: 61, e: 1.0\n",
      "episode: 9/100, score: 61, e: 1.0\n",
      "episode: 10/100, score: 61, e: 1.0\n",
      "episode: 11/100, score: 61, e: 1.0\n",
      "episode: 12/100, score: 61, e: 1.0\n",
      "episode: 13/100, score: 61, e: 1.0\n",
      "episode: 14/100, score: 61, e: 1.0\n",
      "episode: 15/100, score: 61, e: 1.0\n",
      "episode: 16/100, score: 61, e: 0.96\n",
      "episode: 17/100, score: 61, e: 0.91\n",
      "episode: 18/100, score: 61, e: 0.85\n",
      "episode: 19/100, score: 61, e: 0.8\n",
      "episode: 20/100, score: 61, e: 0.75\n",
      "episode: 21/100, score: 61, e: 0.71\n",
      "episode: 22/100, score: 61, e: 0.67\n",
      "episode: 23/100, score: 61, e: 0.63\n",
      "episode: 24/100, score: 61, e: 0.59\n",
      "episode: 25/100, score: 61, e: 0.56\n",
      "episode: 26/100, score: 61, e: 0.52\n",
      "episode: 27/100, score: 61, e: 0.49\n",
      "episode: 28/100, score: 61, e: 0.46\n",
      "episode: 29/100, score: 61, e: 0.44\n",
      "episode: 30/100, score: 61, e: 0.41\n",
      "episode: 31/100, score: 61, e: 0.39\n",
      "episode: 32/100, score: 61, e: 0.36\n",
      "episode: 33/100, score: 61, e: 0.34\n",
      "episode: 34/100, score: 61, e: 0.32\n",
      "episode: 35/100, score: 61, e: 0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(8, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='model1')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = AgentEnv()\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.EPISODES = 100\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 1/(1+0.02)    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
    "                    if i == 500:\n",
    "                        print(\"Saving trained model as model.h5\")\n",
    "                        self.save(\"./model.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"./model.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent()\n",
    "    agent.run()\n",
    "    agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
